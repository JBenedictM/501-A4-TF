{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "501_A4_P2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JBenedictM/501-A4-TF/blob/master/501_A4_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1o4PRH6KJuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ughi_1E4dp",
        "colab_type": "text"
      },
      "source": [
        "Upload notMNIST.npz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4eVDT0IKKbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/notMNIST.npz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmlX83--KT6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"--Get data--\")\n",
        "# path is defined by the cell above\n",
        "with np.load(path, allow_pickle=True) as f:\n",
        "    x_train, y_train = f['x_train'], f['y_train']\n",
        "    x_test, y_test = f['x_test'], f['y_test']\n",
        "\n",
        "print(\"--Process data--\")\n",
        "print(len(y_train))\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "#shape = (batch_size, height, width, depth)\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        " \n",
        " # new model employes more convolution layers \n",
        " # each layer also contains more kernels\n",
        "print(\"--Make model--\")\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, input_shape=(28, 28, 1), kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "  tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dropout(0.50),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.50),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--Fit model--\")\n",
        "val_loss_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=20)\n",
        "checkpoint_val = tf.keras.callbacks.ModelCheckpoint(\"best_notmnist_model.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
        "checkpoint_train = tf.keras.callbacks.ModelCheckpoint(\"best_notmnist_model.h5\", monitor=\"loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
        "\n",
        "cb = [val_loss_callback, checkpoint_val, checkpoint_train]\n",
        "\n",
        "model.fit(x_train, y_train, epochs=99999999,  verbose=1, validation_data=(x_test, y_test), callbacks=cb)\n",
        "\n",
        "print(\"--Evaluate model--\")\n",
        "model_loss, model_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "model.summary()\n",
        "print(f\"Model Loss:    {model_loss:.2f}\")\n",
        "print(f\"Model Accuray: {model_acc*100:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqOYMbbzEwUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save Model to notMNIST.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ZLaLRVEweP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('best_notmnist_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}